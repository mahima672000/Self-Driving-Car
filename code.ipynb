{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Self Driving Car"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem Definition\n",
    "We are here building a minimal version of self driving car. Here, we have a front camera view. This will transfer input to the computer. Then Deep Learning algorithm in computer predicts the steering angle to avoid all sorts of collisions. Predicting steering angle can be thought of as a regression problem. We will feed images to Convolutional Neural Network and the label will be the steering angle in that image. Model will learn the steering angle from the as per the turns in the image and will finally predicts steering angle for unknown images."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset\n",
    "\n",
    "Refer this: https://github.com/SullyChen/Autopilot-TensorFlow\n",
    "\n",
    "There are total 45406 images in the dataset along with their steering angles. We will split the dataset into train and test in a ratio of 80:20 <b>sequentially</b>."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\GauravP\\Anaconda3\\lib\\site-packages\\h5py\\__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy import pi\n",
    "import cv2\n",
    "import scipy.misc\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Reading images from file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_FOLDER = \"../driving_dataset/\"\n",
    "DATA_FILE = os.path.join(DATA_FOLDER, \"data.txt\")\n",
    "\n",
    "x = []\n",
    "y = []\n",
    "\n",
    "train_batch_pointer = 0\n",
    "test_batch_pointer = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "45406 45406\n"
     ]
    }
   ],
   "source": [
    "with open(DATA_FILE) as f:\n",
    "    for line in f:\n",
    "        image_name, angle = line.split()\n",
    "        \n",
    "        image_path = os.path.join(DATA_FOLDER, image_name)\n",
    "        x.append(image_path)\n",
    "        \n",
    "        angle_radians = float(angle) * (pi / 180)  #converting angle into radians\n",
    "        y.append(angle_radians)\n",
    "y = np.array(y)\n",
    "print(str(len(x))+\" \"+str(len(y)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(36324, 36324, 9082, 9082)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "split_ratio = int(len(x) * 0.8)\n",
    "\n",
    "train_x = x[:split_ratio]\n",
    "train_y = y[:split_ratio]\n",
    "\n",
    "test_x = x[split_ratio:]\n",
    "test_y = y[split_ratio:]\n",
    "\n",
    "len(train_x), len(train_y), len(test_x), len(test_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 720x504 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "fig = plt.figure(figsize = (10, 7))\n",
    "plt.hist(train_y, bins = 50, histtype = \"step\")\n",
    "plt.hist(test_y, bins = 50, histtype = \"step\")\n",
    "plt.title(\"Steering Wheel angle in train and test\")\n",
    "plt.xlabel(\"Angle\")\n",
    "plt.ylabel(\"Bin count\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Above histogram plot clearly shows that most of the values list on 0. This is obvious as well as most of the time car runs on straight road so therefore, steering wheel angle is 0 most of the time during driving."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Writing function for creating batch of images for training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loadTrainBatch(batch_size):\n",
    "    global train_batch_pointer\n",
    "    x_result = []\n",
    "    y_result = []\n",
    "    for i in range(batch_size):\n",
    "        read_image = cv2.imread(train_x[(train_batch_pointer + i) % len(train_x)]) #here % len(train_x) is used to make sure that\n",
    "        #\"train_batch_pointer + i\" should not cross the number of train images. As soon as the value of \"train_batch_pointer\" is\n",
    "        #equal to number of train images then it will again start reading the train images from the beginning means from 0th\n",
    "        #index onwards.\n",
    "        read_image_road = read_image[-150:] #here, we are taking only the lower part of the images where there is a road in the\n",
    "        #image. As, we are concern only with the curves of the road to predict angles so therefore, we are discarding the upper\n",
    "        #part of the image. Hence, here -\"150\" is equivalent to the last 150 matrix pixels of the image.\n",
    "        read_image_resize = cv2.resize(read_image_road, (200, 66)) #After, resizing, each image will be of size (66, 200, 3). \n",
    "        #now since we have kept only the last 150 matrices in the image so the size of our image is now (150, 455, 3). \n",
    "        #Now 455/150 = 3.0303. Also 200/66 = 3.0303. Hence, here we are keeping the aspect ratio of images same.\n",
    "        read_image_final = read_image_resize/255.0  #here, we are normalizing the images\n",
    "        \n",
    "        x_result.append(read_image_final) #finally appending the image pixel matrix\n",
    "        \n",
    "        y_result.append(train_y[(train_batch_pointer + i) % len(train_y)]) #appending corresponding labels\n",
    "        \n",
    "    train_batch_pointer += batch_size\n",
    "        \n",
    "    return x_result, y_result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loadTestBatch(batch_size):\n",
    "    global test_batch_pointer\n",
    "    x_result = []\n",
    "    y_result = []\n",
    "    for i in range(batch_size):\n",
    "        read_image = cv2.imread(test_x[(test_batch_pointer + i) % len(test_x)]) #here % len(test_x) is used to make sure that\n",
    "        #\"test_batch_pointer + i\" should not cross the number of test images. As soon as the value of \"test_batch_pointer\" is\n",
    "        #equal to number of test images then it will again start reading the test images from the beginning means from 0th\n",
    "        #index onwards.\n",
    "        read_image_road = read_image[-150:] #here, we are taking only the lower part of the images where there is a road in the\n",
    "        #image. As, we are concern only with the curves of the road to predict angles so therefore, we are discarding the upper\n",
    "        #part of the image. Hence, here -\"150\" is equivalent to the last 150 matrix pixels of the image.\n",
    "        read_image_resize = cv2.resize(read_image_road, (200, 66)) #After, resizing, each image will be of size (66, 200, 3). \n",
    "        #now since we have kept only the last 150 matrices in the image so the size of our image is now (150, 455, 3). \n",
    "        #Now 455/150 = 3.0303. Also 200/66 = 3.0303. Hence, here we are keeping the aspect ratio of images same.\n",
    "        read_image_final = read_image_resize/255.0  #here, we are normalizing the images\n",
    "        \n",
    "        x_result.append(read_image_final) #finally appending the image pixel matrix\n",
    "        \n",
    "        y_result.append(test_y[(test_batch_pointer + i) % len(test_y)]) #appending corresponding labels\n",
    "        \n",
    "    test_batch_pointer += batch_size\n",
    "        \n",
    "    return x_result, y_result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Making Model Architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def weightVariable(shape):\n",
    "    initial = tf.truncated_normal(shape = shape, stddev = 0.1)\n",
    "    return tf.Variable(initial) \n",
    "\n",
    "def bias_variable(shape):\n",
    "    initial = tf.constant(0.1, shape=shape)\n",
    "    return tf.Variable(initial)\n",
    "\n",
    "def convolution(previous_input, filter_input, strides):\n",
    "    return tf.nn.conv2d(previous_input, filter_input, strides = [1, strides, strides, 1], padding = \"VALID\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_input = tf.placeholder(tf.float32, shape = [None, 66, 200, 3], name = \"Plc_1\")\n",
    "y_true = tf.placeholder(tf.float32, name = \"Plc_2\")\n",
    "\n",
    "input_image = x_input"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src = \"Model_Architecture.png\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Convolution Layers\n",
    "#First convolution layer\n",
    "W_Conv1 = weightVariable([5,5,3,24])\n",
    "B_Conv1 = bias_variable([24])\n",
    "Conv1 = tf.nn.relu(convolution(input_image, W_Conv1, 2) + B_Conv1)\n",
    "#strides = 2\n",
    "#Output size: 31*98*24\n",
    "\n",
    "#Second convolution layer\n",
    "W_Conv2 = weightVariable([5,5,24,36])\n",
    "B_Conv2 = bias_variable([36])\n",
    "Conv2 = tf.nn.relu(convolution(Conv1, W_Conv2, 2) + B_Conv2)\n",
    "#strides = 2\n",
    "#Output size: 14*47*36\n",
    "\n",
    "#Third convolution layer\n",
    "W_Conv3 = weightVariable([5,5,36,48])\n",
    "B_Conv3 = bias_variable([48])\n",
    "Conv3 = tf.nn.relu(convolution(Conv2, W_Conv3, 2) + B_Conv3)\n",
    "#strides = 2\n",
    "#Output size: 5*22*48\n",
    "\n",
    "#Fourth convolution layer\n",
    "W_Conv4 = weightVariable([3,3,48,64])\n",
    "B_Conv4 = bias_variable([64])\n",
    "Conv4 = tf.nn.relu(convolution(Conv3, W_Conv4, 1) + B_Conv4)\n",
    "#strides = 1\n",
    "#Output size: 3*20*64\n",
    "\n",
    "\n",
    "#Fifth convolution layer\n",
    "W_Conv5 = weightVariable([3,3,64,64])\n",
    "B_Conv5 = bias_variable([64])\n",
    "Conv5 = tf.nn.relu(convolution(Conv4, W_Conv5, 1) + B_Conv5)\n",
    "#strides = 1\n",
    "#Output size: 1*18*64\n",
    "\n",
    "#Fully-Connected Dense Layers\n",
    "keep_prob = tf.placeholder(tf.float32)\n",
    "#First FC-Dense\n",
    "#Input = 1*18*64 = 1152\n",
    "W_FC1 = weightVariable([1152, 1164])\n",
    "B_FC1 = bias_variable([1164])\n",
    "FC1_Flatten = tf.reshape(Conv5, [-1, 1152]) #here, -1 indicates 1. It means that the shape of FC1_Flatten will be 1*1152\n",
    "Output_FC1 = tf.nn.relu(tf.matmul(FC1_Flatten, W_FC1) + B_FC1) #so, here shape of FC1_Flatten is 1*1152 and shape of W_FC1 will\n",
    "#be 1152*1164. Therefore, there will be a matrix multiplication of matrices: (1*1152) * (1152*1164) = (1*1164).\n",
    "Output_FC1_drop = tf.nn.dropout(Output_FC1, keep_prob)\n",
    "\n",
    "#Second FC-Dense\n",
    "#Input = 1*1164 = 1164\n",
    "W_FC2 = weightVariable([1164, 100])\n",
    "B_FC2 = bias_variable([100])\n",
    "Output_FC2 = tf.nn.relu(tf.matmul(Output_FC1_drop, W_FC2) + B_FC2) #so, here shape of Output_FC1_drop is 1*1164 and shape of \n",
    "#W_FC2 will be 1164*100. Therefore, there will be a matrix multiplication of matrices: (1*1164) * (1164*100) = (1*100).\n",
    "Output_FC2_drop = tf.nn.dropout(Output_FC2, keep_prob)\n",
    "\n",
    "#Third FC-Dense\n",
    "#Input = 1*100 = 100\n",
    "W_FC3 = weightVariable([100, 50])\n",
    "B_FC3 = bias_variable([50])\n",
    "Output_FC3 = tf.nn.relu(tf.matmul(Output_FC2_drop, W_FC3) + B_FC3) #so, here shape of Output_FC2_drop is 1*100 and shape of \n",
    "#W_FC3 will be 100*50. Therefore, there will be a matrix multiplication of matrices: (1*100) * (100*50) = (1*50).\n",
    "Output_FC3_drop = tf.nn.dropout(Output_FC3, keep_prob)\n",
    "\n",
    "#Fourth FC-Dense\n",
    "#Input = 1*50 = 50\n",
    "W_FC4 = weightVariable([50, 10])\n",
    "B_FC4 = bias_variable([10])\n",
    "Output_FC4 = tf.nn.relu(tf.matmul(Output_FC3_drop, W_FC4) + B_FC4) #so, here shape of Output_FC3_drop is 1*50 and shape of \n",
    "#W_FC4 will be 50*10. Therefore, there will be a matrix multiplication of matrices: (1*50) * (50*10) = (1*10).\n",
    "Output_FC4_drop = tf.nn.dropout(Output_FC4, keep_prob)\n",
    "\n",
    "#Final Output to one neuron with linear/identity function\n",
    "#Input = 1*10 = 10\n",
    "W_FC5 = weightVariable([10, 1])\n",
    "B_FC5 = bias_variable([1])\n",
    "y_predicted = tf.identity(tf.matmul(Output_FC4_drop, W_FC5) + B_FC5) #so, here shape of Output_FC4_drop is 1*10 and shape of \n",
    "#W_FC5 will be 10*1. Therefore, there will be a matrix multiplication of matrices: (1*10) * (10*1) = (1*1). Since, this is a \n",
    "#regression problem so we have applied identity fuction in the end. We can also apply \"atan\" function here. If computational\n",
    "#power is available then the model should be tested with both identity and atan functions. In the end, that function should be\n",
    "#considered which gives better result."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Training the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "SAVEDIR = \"../Saver/\"\n",
    "sess = tf.InteractiveSession()\n",
    "\n",
    "L2NormConst = 0.001\n",
    "train_vars = tf.trainable_variables() #it will return all the variables. Here, all the weights and biases are variables which\n",
    "#are trainable.\n",
    "\n",
    "loss = tf.reduce_mean(tf.square(tf.subtract(y_true, y_predicted))) + tf.add_n([tf.nn.l2_loss(w) for w in train_vars]) * L2NormConst\n",
    "#since this is a regression problem so above loss is mean-squared-error loss\n",
    "train_step = tf.train.AdamOptimizer(learning_rate = 10**-4).minimize(loss)\n",
    "sess.run(tf.global_variables_initializer())\n",
    "\n",
    "saver = tf.train.Saver()\n",
    "\n",
    "epochs = 30\n",
    "batch_size = 100\n",
    "epoch_number, train_loss, test_loss,  = [], [], []\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    train_avg_loss = 0\n",
    "    test_avg_loss = 0\n",
    "    te_loss_old = 10000  #any big number can be given\n",
    "    \n",
    "    for i in range(int(len(x)/batch_size)):\n",
    "        train_batch_x, train_batch_y = loadTrainBatch(batch_size)\n",
    "        train_step.run(feed_dict = {x_input: train_batch_x, y_true: train_batch_y, keep_prob: 0.8})\n",
    "        tr_loss = loss.eval(feed_dict = {x_input: train_batch_x, y_true: train_batch_y, keep_prob: 1.0})\n",
    "        train_avg_loss += tr_loss / batch_size\n",
    "    \n",
    "        test_batch_x, test_batch_y = loadTestBatch(batch_size)\n",
    "        te_loss_new = loss.eval(feed_dict = {x_input: test_batch_x, y_true: test_batch_y, keep_prob: 1.0})\n",
    "        test_avg_loss += te_loss_new / batch_size\n",
    "        \n",
    "        if te_loss_new < te_loss_old:\n",
    "            print(\"Epoch: {}, Train_Loss: {}, Test_Loss: {} *\".format(epoch+1, tr_loss, te_loss_new))\n",
    "        else:\n",
    "            print(\"Epoch: {}, Train_Loss: {}, Test_Loss: {}\".format(epoch+1, tr_loss, te_loss_new))\n",
    "        te_loss_old = te_loss_new\n",
    "        \n",
    "        if (i+1) % batch_size == 0:\n",
    "            if not os.path.exists(SAVEDIR):\n",
    "                os.makedirs(SAVEDIR)\n",
    "            save_path = os.path.join(SAVEDIR, \"model.ckpt\")\n",
    "            saver.save(sess = sess, save_path = save_path)\n",
    "            print(\"Model saved at location {} at epoch {}\".format(save_path, epoch + 1))\n",
    "        \n",
    "    epoch_number.append(epoch)\n",
    "    train_loss.append(train_avg_loss)\n",
    "    test_loss.append(test_avg_loss)\n",
    "    \n",
    "#creating dataframe and record all the losses and accuracies at each epoch\n",
    "log_frame = pd.DataFrame(columns = [\"Epoch\", \"Train Loss\", \"Test Loss\"])\n",
    "log_frame[\"Epoch\"] = epoch_number\n",
    "log_frame[\"Train Loss\"] = train_loss\n",
    "log_frame[\"Test Loss\"] = test_loss\n",
    "log_frame.to_csv(os.path.join(SAVEDIR, \"log.csv\"), index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Epoch</th>\n",
       "      <th>Train Loss</th>\n",
       "      <th>Test Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>25.441976</td>\n",
       "      <td>24.565600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>18.587419</td>\n",
       "      <td>18.186967</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>14.609151</td>\n",
       "      <td>14.172622</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>11.957517</td>\n",
       "      <td>11.540807</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>10.433561</td>\n",
       "      <td>9.667840</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>5</td>\n",
       "      <td>8.524213</td>\n",
       "      <td>8.287085</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>6</td>\n",
       "      <td>7.396868</td>\n",
       "      <td>7.110160</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>7</td>\n",
       "      <td>6.448679</td>\n",
       "      <td>6.113556</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>8</td>\n",
       "      <td>5.962348</td>\n",
       "      <td>5.384088</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>9</td>\n",
       "      <td>4.816765</td>\n",
       "      <td>4.692729</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>10</td>\n",
       "      <td>4.327291</td>\n",
       "      <td>4.103290</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>11</td>\n",
       "      <td>3.831057</td>\n",
       "      <td>3.698865</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>12</td>\n",
       "      <td>3.768895</td>\n",
       "      <td>3.492390</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>13</td>\n",
       "      <td>3.195319</td>\n",
       "      <td>3.057956</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>14</td>\n",
       "      <td>2.939912</td>\n",
       "      <td>2.794230</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>15</td>\n",
       "      <td>2.779324</td>\n",
       "      <td>2.687055</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>16</td>\n",
       "      <td>2.948726</td>\n",
       "      <td>2.751356</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>17</td>\n",
       "      <td>2.449967</td>\n",
       "      <td>2.380851</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>18</td>\n",
       "      <td>2.387339</td>\n",
       "      <td>2.277248</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>19</td>\n",
       "      <td>2.271806</td>\n",
       "      <td>2.301014</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>20</td>\n",
       "      <td>2.451325</td>\n",
       "      <td>2.404977</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>21</td>\n",
       "      <td>2.111570</td>\n",
       "      <td>2.093948</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>22</td>\n",
       "      <td>2.023531</td>\n",
       "      <td>2.142157</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>23</td>\n",
       "      <td>1.930803</td>\n",
       "      <td>2.180721</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>24</td>\n",
       "      <td>2.234076</td>\n",
       "      <td>2.202252</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>25</td>\n",
       "      <td>1.851828</td>\n",
       "      <td>2.028983</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>26</td>\n",
       "      <td>1.775510</td>\n",
       "      <td>2.035361</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>27</td>\n",
       "      <td>1.739461</td>\n",
       "      <td>1.888012</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>28</td>\n",
       "      <td>2.007302</td>\n",
       "      <td>2.063404</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>29</td>\n",
       "      <td>1.615547</td>\n",
       "      <td>1.970689</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    Epoch  Train Loss  Test Loss\n",
       "0       0   25.441976  24.565600\n",
       "1       1   18.587419  18.186967\n",
       "2       2   14.609151  14.172622\n",
       "3       3   11.957517  11.540807\n",
       "4       4   10.433561   9.667840\n",
       "5       5    8.524213   8.287085\n",
       "6       6    7.396868   7.110160\n",
       "7       7    6.448679   6.113556\n",
       "8       8    5.962348   5.384088\n",
       "9       9    4.816765   4.692729\n",
       "10     10    4.327291   4.103290\n",
       "11     11    3.831057   3.698865\n",
       "12     12    3.768895   3.492390\n",
       "13     13    3.195319   3.057956\n",
       "14     14    2.939912   2.794230\n",
       "15     15    2.779324   2.687055\n",
       "16     16    2.948726   2.751356\n",
       "17     17    2.449967   2.380851\n",
       "18     18    2.387339   2.277248\n",
       "19     19    2.271806   2.301014\n",
       "20     20    2.451325   2.404977\n",
       "21     21    2.111570   2.093948\n",
       "22     22    2.023531   2.142157\n",
       "23     23    1.930803   2.180721\n",
       "24     24    2.234076   2.202252\n",
       "25     25    1.851828   2.028983\n",
       "26     26    1.775510   2.035361\n",
       "27     27    1.739461   1.888012\n",
       "28     28    2.007302   2.063404\n",
       "29     29    1.615547   1.970689"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "frame = pd.read_csv(os.path.join(SAVEDIR, \"log.csv\"))\n",
    "frame"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Making Predictions from the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from ../Saver/model.ckpt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\GauravP\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\client\\session.py:1711: UserWarning: An interactive session is already active. This can cause out-of-memory errors in some cases. You must explicitly call `InteractiveSession.close()` to release resources held by the other session(s).\n",
      "  warnings.warn('An interactive session is already active. This can '\n"
     ]
    }
   ],
   "source": [
    "sess = tf.InteractiveSession()\n",
    "saver = tf.train.Saver()\n",
    "saver.restore(sess, \"../Saver/model.ckpt\")\n",
    "\n",
    "img = cv2.imread('steering_wheel_image.jpg', 0) #here, second parameter '0' specifies that img.shape will return only height and\n",
    "#width of the image and not the number of channels. It is a colored image so number of channels = 3, which it will not return.\n",
    "rows, cols = img.shape\n",
    "\n",
    "i = 0\n",
    "while(cv2.waitKey(60) != ord(\"q\")):\n",
    "    full_image = cv2.imread(test_x[i])\n",
    "    cv2.imshow('Frame Window', full_image)\n",
    "    image = ((cv2.resize(full_image[-150:], (200, 66)) / 255.0).reshape((1, 66, 200, 3)))\n",
    "    degrees = sess.run(y_predicted, feed_dict = {x_input: image, keep_prob: 1.0})[0][0] *180 / pi #here, we have converted the\n",
    "    #predicted degrees from radians to degrees.\n",
    "    M = cv2.getRotationMatrix2D((cols/2,rows/2), -degrees, 1) #this function rotate the image by a given degrees.\n",
    "    dst = cv2.warpAffine(src = img, M = M, dsize = (cols, rows)) #warpAffine function applies rotation to the image\n",
    "    cv2.imshow(\"Steering Wheel\", dst)\n",
    "    i += 1\n",
    "\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run the file \"Visualize_Output.py\" at command prompt to visualize the output better."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
